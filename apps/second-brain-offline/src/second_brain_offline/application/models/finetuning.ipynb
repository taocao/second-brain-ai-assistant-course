{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CARcuhqBN62M"
      },
      "source": [
        "# Training Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEWWCtEdN62O"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fzVXPFaDN62O"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth==2025.1.6 comet_ml==3.48.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFlWfMwe667-"
      },
      "source": [
        "## Prepare environment\n",
        "\n",
        "If you want to upload your dataset to Hugging Face, enter your Hugging Token token (it's free).\n",
        "\n",
        "If you want to use Comet as an experiment tracker enter your Comet API Key (it's free)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "258SGN2k667-",
        "outputId": "e0e8c5fa-d218-46bd-bf3a-6885bd5af9b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Hugging Face token. Press Enter to skip: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Is Hugging Face enabled? 'True'\n",
            "Enter your Comet API key. Press Enter to skip: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Is Comet enabled? 'True'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "from getpass import getpass\n",
        "\n",
        "hf_token = getpass(\"Enter your Hugging Face token. Press Enter to skip: \")\n",
        "enable_hf = bool(hf_token)\n",
        "print(f\"Is Hugging Face enabled? '{enable_hf}'\")\n",
        "\n",
        "comet_api_key = getpass(\"Enter your Comet API key. Press Enter to skip: \")\n",
        "enable_comet = bool(comet_api_key)\n",
        "comet_project_name = \"second-brain-course\"\n",
        "print(f\"Is Comet enabled? '{enable_comet}'\")\n",
        "\n",
        "if enable_hf:\n",
        "    os.environ[\"HF_TOKEN\"] = hf_token\n",
        "if enable_comet:\n",
        "    os.environ[\"COMET_API_KEY\"] = comet_api_key\n",
        "    os.environ[\"COMET_PROJECT_NAME\"] = comet_project_name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXTnKL5TZ7ET"
      },
      "source": [
        "## Global variables\n",
        "\n",
        "Make sure you have an Nvidia GPU active. You can choose it from the **Runtime** tab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkNpzC_z-IZ4",
        "outputId": "f7152028-ddae-408d-e155-f1a0688e0df3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU type:\n",
            "NVIDIA L4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def get_gpu_info() -> str | None:\n",
        "    \"\"\"Gets GPU device name if available.\n",
        "\n",
        "    Returns:\n",
        "        str | None: Name of the GPU device if available, None if no GPU is found.\n",
        "    \"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        return None\n",
        "\n",
        "    gpu_name = torch.cuda.get_device_properties(0).name\n",
        "\n",
        "    return gpu_name\n",
        "\n",
        "\n",
        "active_gpu_name = get_gpu_info()\n",
        "\n",
        "print(\"GPU type:\")\n",
        "print(active_gpu_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_id = input(\"Enter your Hugging Face dataset_id (which you generated in Lesson 3). Hit enter to use our precomputed version: \") or \"pauliusztin/second_brain_course_summarization_task\"\n",
        "print(f\"{dataset_id=}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT437-WQlzzk",
        "outputId": "20522bec-aca7-472d-f55f-6f997b8fb8fd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Hugging Face dataset_id (which you generated in Lesson 3). Hit enter to use our precomputed version: \n",
            "dataset_id='pauliusztin/second_brain_course_summarization_task'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on your GPU type, we must pick different variables, as training in 4bit (QLoRA) takes substantially longer than training in 16bit (LoRA). Thus, if you have a T4 Nivia GPU, which is available in Google's Colab free tier, to avoid waiting an eternity for the fine-tuning to complete, we will train for fewer steps (on T4, we cannot train with LoRA without encountering issues while fine-tuning)."
      ],
      "metadata": {
        "id": "fom9fNDrkqwF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TAD8546zZ9lX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d395bae2-b2bf-45fc-cd19-ccf548960815"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Parameters ---\n",
            "max_steps=25\n",
            "load_in_4bit=False\n",
            "dtype=None\n"
          ]
        }
      ],
      "source": [
        "max_seq_length = 4096  # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = (\n",
        "    None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        ")\n",
        "if active_gpu_name and \"T4\" in active_gpu_name:\n",
        "    load_in_4bit = True  # Use 4bit quantization to reduce memory usage.\n",
        "    max_steps = 25  # Reduce training steps to avoiding waiting too long.\n",
        "elif active_gpu_name and (\"A100\" in active_gpu_name or \"L4\" in active_gpu_name):\n",
        "    load_in_4bit = False  # Disable 4bit quantization for faster training.\n",
        "    max_steps = 500  # As we train without 4bit quantization, we can train for more steps without waiting too long.\n",
        "elif active_gpu_name:\n",
        "    load_in_4bit = False  # Disable 4bit quantization for faster training.\n",
        "    max_steps = 250  # As we train without 4bit quantization, we can train for more steps without waiting too long.\n",
        "else:\n",
        "    raise ValueError(\"No Nvidia GPU found.\")\n",
        "\n",
        "print(\"--- Parameters ---\")\n",
        "print(f\"{max_steps=}\")\n",
        "print(f\"{load_in_4bit=}\")\n",
        "print(f\"{dtype=}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8THT8GCcN62P"
      },
      "source": [
        "## Load LLM using Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192,
          "referenced_widgets": [
            "76190c73b3ef4adea553107e29eec382",
            "2cefba26286549b0a79bee0c49ad5718",
            "324a04724d854acd8ce2f711ed78b61b",
            "0d7fad603e784644b71263662efcc86c",
            "e0d978b82000482782fb31b4d0fdb571",
            "425114fe54594f2693231501a4872b70",
            "23ebded49cf843ba876be4cc8148268e",
            "62f04293d83e43818e7c9a6c65aae64b",
            "d62fa06d371041108d746bdafa5409f6",
            "7b6e61efaf4b496b8346a0c2e1247e4a",
            "4bddcd59b2624581af78cb8c5970fa16"
          ]
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "6af19d0d-299d-4b3d-f73e-deebe4a2b2ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.1.6: Fast Llama patching. Transformers: 4.47.1.\n",
            "   \\\\   /|    GPU: NVIDIA L4. Max memory: 22.161 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76190c73b3ef4adea553107e29eec382"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "base_model = \"Meta-Llama-3.1-8B-Instruct\"  # or unsloth/Qwen2.5-7B-Instruct\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=f\"unsloth/{base_model}\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "81bef7b3-bb4d-4d9d-c7c2-f433305b9482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.1.6 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
        "    bias=\"none\",  # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
        "    random_state=3407,\n",
        "    use_rslora=False,  # We support rank stabilized LoRA\n",
        "    loftq_config=None,  # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "### Data Preperation\n",
        "\n",
        "We now use the Alpaca format to map the instruct dataset into input prompts.\n",
        "\n",
        "**[NOTE]** Remember to add the **EOS_TOKEN** to the tokenized output!! Otherwise you'll get infinite generations!\n",
        "\n",
        "If you want to use the `llama-3` template for ShareGPT datasets, try our conversational [notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing).\n",
        "\n",
        "For text completions like novel writing, try this [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vYOshK3saLD_"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "You are a helpful assistant specialized in summarizing documents. Generate a concise TL;DR summary in markdown format having a maximum of 512 characters of the key findings from the provided documents, highlighting the most significant insights\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
        "\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    inputs = examples[\"instruction\"]\n",
        "    outputs = examples[\"answer\"]\n",
        "    texts = []\n",
        "    for input, output in zip(inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(input, output) + EOS_TOKEN\n",
        "\n",
        "        texts.append(text)\n",
        "    return {\n",
        "        \"text\": texts,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(dataset_id)\n",
        "dataset = dataset.map(\n",
        "    formatting_prompts_func,\n",
        "    batched=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=True,  # Can make training 5x faster for short sequences.\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        # num_train_epochs=1,  # Set this for 1 full training run, while commenting out 'max_steps'.\n",
        "        max_steps=max_steps,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"comet_ml\" if enable_comet else \"none\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "c1a7a7ef-a59a-4074-fe93-8e9aeb9a54fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = NVIDIA L4. Max memory = 22.161 GB.\n",
            "15.729 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "24f3bf27-2ae0-4cd5-9f35-2e89e37023af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 1,467 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 25\n",
            " \"-____-\"     Number of trainable parameters = 41,943,040\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch, keras, tensorflow.\n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/iusztinpaul/second-brain-course/9d93351ca58e4e76951797f4ec7f5c1d\n",
            "\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/content' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/25 15:58, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.342100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.222200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.294200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.428300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.534600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.595300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.415100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.411700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.429000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.186300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.404000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.520600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.304400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.767800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.358800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.459700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.324200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.352800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.328200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.481000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.311400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.685900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.402600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.170500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.494500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "d4bc58be-9875-4b6d-d977-6cdd3ac04561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1001.4489 seconds used for training.\n",
            "16.69 minutes used for training.\n",
            "Peak reserved memory = 17.18 GB.\n",
            "Peak reserved memory for training = 1.451 GB.\n",
            "Peak reserved memory % of max memory = 77.524 %.\n",
            "Peak reserved memory for training % of max memory = 6.548 %.\n"
          ]
        }
      ],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime'] / 60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!\n",
        "\n",
        " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "e2pEuRb1r2Vg"
      },
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "\n",
        "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "\n",
        "\n",
        "def generate_text(instruction, streaming: bool = True, trim_input_message: bool = False):\n",
        "  message = alpaca_prompt.format(\n",
        "      instruction,\n",
        "      \"\",  # output - leave this blank for generation!\n",
        "  )\n",
        "  inputs = tokenizer([message], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "  if streaming:\n",
        "    return model.generate(**inputs, streamer=text_streamer, max_new_tokens=256, use_cache=True)\n",
        "  else:\n",
        "    output_tokens = model.generate(**inputs, max_new_tokens=256, use_cache=True)\n",
        "    output = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)[0]\n",
        "\n",
        "    if trim_input_message:\n",
        "      return output[len(message) :]\n",
        "    else:\n",
        "      return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(dataset[\"validation\"][0][\"instruction\"], streaming=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUHmeVNef61r",
        "outputId": "267d8521-e7c6-442c-88e8-7ce083de0b61"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "You are a helpful assistant specialized in summarizing documents. Generate a concise TL;DR summary in markdown format having a maximum of 512 characters of the key findings from the provided documents, highlighting the most significant insights\n",
            "\n",
            "### Input:\n",
            "[![](assets/figures/nvidia.svg) Toronto AI Lab  ](https://research.nvidia.com/labs/toronto-ai/)\n",
            "\n",
            "# \n",
            "\n",
            "Align your Latents:High-Resolution Video Synthesis with Latent Diffusion Models\n",
            "\n",
            "[Andreas Blattmann1 *,â€ ](https://twitter.com/andi_blatt) [Robin Rombach1 *,â€ ](https://twitter.com/robrombach) [Huan Ling2,3,4 *](https://www.cs.toronto.edu/~linghuan/) [Tim Dockhorn2,3,5 *,â€ ](https://timudk.github.io/) [Seung Wook Kim2,3,4](https://seung-kim.github.io/seungkim/) [Sanja Fidler2,3,4](https://www.cs.toronto.edu/~fidler/) [Karsten Kreis2](https://karstenkreis.github.io/)\n",
            "\n",
            "1 LMU Munich, 2 NVIDIA, 3 Vector Institute, 4 University of Toronto, 5 University of Waterloo\n",
            "\n",
            "* Equal contribution. â€  Andreas, Robin and Tim did the work during internships at NVIDIA.\n",
            "\n",
            "**IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2023**\n",
            "\n",
            "[Read the paper](https://arxiv.org/abs/2304.08818)\n",
            "\n",
            "Two pandas discussing an academic paper.\n",
            "\n",
            "A storm trooper vacuuming the beach.\n",
            "\n",
            "An animated painting of fluffy white clouds moving in sky.\n",
            "\n",
            "Fireworks.\n",
            "\n",
            "Turtle swimming in ocean.\n",
            "\n",
            "A squirrel eating a burger.\n",
            "\n",
            "Beer pouring into glass, low angle video shot.\n",
            "\n",
            "Sunset time lapse at the beach with moving clouds and colors in the sky, 4k, high resolution.\n",
            "\n",
            "An astronaut feeding ducks on a sunny afternoon, reflection from the water.\n",
            "\n",
            "A cat wearing sunglasses and working as a lifeguard at a pool.\n",
            "\n",
            "Close up of grapes on a rotating table. High definition.\n",
            "\n",
            "An astronaut flying in space, 4k, high resolution.\n",
            "\n",
            "Flying through fantasy landscapes, 4k, high resolution.\n",
            "\n",
            "A koala bear playing piano in the forest.\n",
            "\n",
            "A panda standing on a surfboard in the ocean in sunset, 4k, high resolution.\n",
            "\n",
            "A bigfoot walking in the snowstorm.\n",
            "\n",
            "[Click for more samples](samples.html)\n",
            "\n",
            "## Abstract\n",
            "\n",
            "Latent Diffusion Models (LDMs) enable high-quality image synthesis while avoiding excessive compute demands by training a diffusion model in a compressed lower-dimensional latent space. Here, we apply the LDM paradigm to high-resolution video generation, a particularly resource-intensive task. We first pre-train an LDM on images only; then, we turn the image generator into a video generator by introducing a temporal dimension to the latent space diffusion model and fine-tuning on encoded image sequences, i.e., videos. Similarly, we temporally align diffusion model upsamplers, turning them into temporally consistent video super resolution models. We focus on two relevant real-world applications: Simulation of in-the-wild driving data and creative content creation with text-to-video modeling. In particular, we validate our Video LDM on real driving videos of resolution 512 x 1024, achieving state-of-the-art performance. Furthermore, our approach can easily leverage off-the-shelf pre-trained image LDMs, as we only need to train a temporal alignment model in that case. Doing so, we turn the publicly available, state-of-the-art text-to-image LDM Stable Diffusion into an efficient and expressive text-to-video model with resolution up to 1280 x 2048. We show that the temporal layers trained in this way generalize to different fine-tuned text-to-image LDMs. Utilizing this property, we show the first results for personalized text-to-video generation, opening exciting directions for future content creation. \n",
            "\n",
            "Your browser does not support the video tag. \n",
            "\n",
            "**Animation of temporal video fine-tuning in our Video Latent Diffusion Models (Video LDMs).** We turn pre-trained image diffusion models into temporally consistent video generators. Initially, different samples of a batch synthesized by the model are independent. After temporal video fine-tuning, the samples are temporally aligned and form coherent videos. The stochastic generation processes before and after fine-tuning are visualised for a diffusion model of a one-dimensional toy distribution. For clarity, the figure corresponds to alignment in pixel space. In practice, we perform alignment in LDM's latent space and obtain videos after applying LDM's decoder. \n",
            "\n",
            "## Video Latent Diffusion Models\n",
            "\n",
            "We present Video Latent Diffusion Models (Video LDMs) for computationally efficient high-resolution video generation. To alleviate the intensive compute and memory demands of high-resolution video synthesis, we leverage the LDM paradigm and extend it to video generation. Our Video LDMs map videos into a compressed latent space and model sequences of latent variables corresponding to the video frames (see animation above). We initialize the models from image LDMs and insert temporal layers into the LDMs' denoising neural networks to temporally model encoded video frame sequences. The temporal layers are based on temporal attention as well as 3D convolutions. We also fine-tune the model's decoder for video generation (see figure below). \n",
            "\n",
            "![](./assets/figures/video_ldm_pipeline.png)\n",
            "\n",
            "**Latent diffusion model framework and video fine-tuning of decoder.** _Top:_ During temporal decoder fine-tuning, we process video sequences with a frozen per-frame encoder and enforce temporally coherent reconstructions across frames. We additionally employ a video-aware discriminator. _Bottom:_ in LDMs, a diffusion model is trained in latent space. It synthesizes latent features, which are then transformed through the decoder into images. Note that in practice we model entire videos and video fine-tune the latent diffusion model to generate temporally consistent frame sequences. \n",
            "\n",
            "Our Video LDM initially generates sparse keyframes at low frame rates, which are then temporally upsampled twice by another interpolation latent diffusion model. Moreover, optionally training Video LDMs for video prediction by conditioning on starting frames allows us to generate long videos in an autoregressive manner. To achieve high-resolution generation, we further leverage spatial diffusion model upsamplers and temporally align them for video upsampling. The entire generation stack is shown below. \n",
            "\n",
            "![](./assets/figures/video_ldm_stack.png)\n",
            "\n",
            "**Video LDM Stack.** We first generate sparse key frames. Then we temporally interpolate in two steps with the same interpolation model to achieve high frame rates. These operations use latent diffusion models (LDMs) that share the same image backbone. Finally, the latent video is decoded to pixel space and optionally a video upsampler diffusion model is applied. \n",
            "\n",
            "**Applications.** We validate our approach on two relevant but distinct applications: Generation of in-the-wild driving scene videos and creative content creation with text-to-video modeling. For driving video synthesis, our Video LDM enables generation of temporally coherent, multiple minute long videos at resolution 512 x 1024, achieving state-of-the-art performance. For text-to-video, we demonstrate synthesis of short videos of several seconds lengths with resolution up to 1280 x 2048, leveraging Stable Diffusion as backbone image LDM as well as the Stable Diffusion upscaler. We also explore the convolutional-in-time application of our models as an alternative approach to extend the length of videos. Our main keyframe models only train the newly inserted temporal layers, but do not touch the layers of the backbone image LDM. Because of that the learnt temporal layers can be transferred to other image LDM backbones, for instance to ones that have been fine-tuned with DreamBooth. Leveraging this property, we additionally show initial results for personalized text-to-video generation. \n",
            "\n",
            "## Text-to-Video Synthesis\n",
            "\n",
            "Many generated videos can be found at the top of the page as well as [**here**](samples.html). The generated videos have a resolution of 1280 x 2048 pixels, consist of 113 frames and are rendered at 24 fps, resulting in 4.7 second long clips. Our Video LDM for text-to-video generation is based on Stable Diffusion and has a total of 4.1B parameters, including all components except the CLIP text encoder. Only 2.7B of these parameters are trained on videos. This means that our models are significantly smaller than those of several concurrent works. Nevertheless, we can produce high-resolution, temporally consistent and diverse videos. This can be attributed to the efficient LDM approach. Below is another text-to-video sample, one of our favorites. \n",
            "\n",
            "Your browser does not support the video tag. \n",
            "\n",
            "Text prompt: \"A teddy bear is playing the electric guitar, high definition, 4k.\" \n",
            "\n",
            "**Personalized Video Generation.** We insert the temporal layers that were trained for our Video LDM for text-to-video synthesis into image LDM backbones that we previously fine-tuned on a set of images following [**DreamBooth**](https://dreambooth.github.io/). The temporal layers generalize to the DreamBooth checkpoints, thereby enabling personalized text-to-video generation. \n",
            "\n",
            "![](./assets/text_to_video/dreambooth/cat_db.png)\n",
            "\n",
            "Training images for DreamBooth. \n",
            "\n",
            "Your browser does not support the video tag. \n",
            "\n",
            "Text prompt: \"A _**sks**_ cat playing in the grass.\" \n",
            "\n",
            "Your browser does not support the video tag. \n",
            "\n",
            "Text prompt: \"A _**sks**_ cat getting up.\" \n",
            "\n",
            "![](./assets/text_to_video/dreambooth/opera_db.png)\n",
            "\n",
            "Training images for DreamBooth. \n",
            "\n",
            "Your browser does not support the video tag. \n",
            "\n",
            "Text prompt: \"A _**sks**_ building next to the Eiffel Tower.\" \n",
            "\n",
            "Your browser does not support the video tag. \n",
            "\n",
            "Text prompt: \"Waves crashing against a _**sks**_ building, ominous lighting.\" \n",
            "\n",
            "![](./assets/text_to_video/dreambooth/frog_db.png)\n",
            "\n",
            "Training images for DreamBooth. \n",
            "\n",
            "Your browser does not support the video tag. \n",
            "\n",
            "Text prompt: \"A _**sks**_ frog playing a guitar in a band.\" \n",
            "\n",
            "Your browser does not support the video tag. \n",
            "\n",
            "Text prompt: \"A _**sks**_ frog writing a scientific research paper.\" \n",
            "\n",
            "![](./assets/text_to_video/dreambooth/teapot_db.png)\n",
            "\n",
            "Training images for DreamBooth. \n",
            "\n",
            "Your browser does not support the video tag. \n",
            "\n",
            "Text prompt: \"A _**sks**_ tea pot floating in the ocean.\" \n",
            "\n",
            "Your browser does not support the video tag. \n",
            "\n",
            "Text prompt: \"A _**sks**_ tea pot on top of a building in New York, drone flight, 4k.\" \n",
            "\n",
            "**Convolutional-in-Time Synthesis.** We also explored synthesizing slightly longer videos \"for free\" by applying our learnt temporal layers convolutionally in time. The below videos consist of 175 frames rendered at 24 fps, resulting in 7.3 second long clips. A minor degradation in quality can be observed. \n",
            "\n",
            "Your browser does not support the video tag. \n",
            "\n",
            "Text prompt: \"Teddy bear walking down 5th Avenue, front view, beautiful sunset, close up, high definition, 4k.\" \n",
            "\n",
            "Your browser does not support the video tag. \n",
            "\n",
            "Text prompt: \"Waves crashing against a lone lighthouse, ominous lighting.\" \n",
            "\n",
            "## Driving Scene Video Generation\n",
            "\n",
            "We also train a Video LDM on in-the-wild real driving scene videos and generate videos at 512 x 1024 resolution. Here, we are additionally training prediction models to enable long video generation, allowing us to generate temporally coherent videos that are several minutes long. Below we show four short synthesized videos. Furthermore, several 5 minute long generated videos can be found [**here**](https://drive.google.com/file/d/1xlE079d4QmVZ-kWLZVsIk8iHWWH5wzKO/view?usp=share_link). \n",
            "\n",
            "Your browser does not support the video tag. \n",
            "\n",
            "Your browser does not support the video tag. \n",
            "\n",
            "Your browser does not support the video tag. \n",
            "\n",
            "Your browser does not support the video tag. \n",
            "\n",
            "**Specific Driving Scenario Simulation.** In practice, we may be interested in simulating a specific scene. To this end, we trained a bounding box-conditioned image-only LDM. Leveraging this model, we can place bounding boxes to construct a setting of interest, synthesize a corresponding starting frame, and then generate plausible videos starting from the designed scene. Below, the image on the left hand side is the initial frame that was generated based on the shown bounding boxes. On the right hand side, a video starting from that frame is generated. \n",
            "\n",
            "![](./assets/driving/bbox_scene_1.png)\n",
            "\n",
            "Your browser does not support the video tag. \n",
            "\n",
            "**Multimodal Driving Scenario Prediction.** As another potentially relevant application, we can take the same starting frame and generate multiple plausible rollouts. In the two sets of videos below, synthesis starts from the same initial frame. \n",
            "\n",
            "Your browser does not support the video tag. \n",
            "\n",
            "Your browser does not support the video tag. \n",
            "\n",
            "## Limitations\n",
            "\n",
            "This is an NVIDIA research project, and the data sources used are for research purposes only and not intended for commercial application or use. \n",
            "\n",
            "## Paper\n",
            "\n",
            "[![](assets/figures/video_ldm_paper_preview.png)](https://arxiv.org/abs/2304.08818)\n",
            "\n",
            "**Align your Latents:High-Resolution Video Synthesis with Latent Diffusion Models**\n",
            "\n",
            "Andreas Blattmann*, Robin Rombach*, Huan Ling*, Tim Dockhorn*, Seung Wook Kim, Sanja Fidler, Karsten Kreis\n",
            "\n",
            "_* Equal contribution._\n",
            "\n",
            "_IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023_\n",
            "\n",
            "description [ arXiv version](https://arxiv.org/abs/2304.08818)\n",
            "\n",
            "insert_comment [ BibTeX](assets/blattmann2023videoldm.bib)\n",
            "\n",
            "## Citation\n",
            "\n",
            "```\n",
            "`@inproceedings{blattmann2023videoldm, title={Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models}, author={Blattmann, Andreas and Rombach, Robin and Ling, Huan and Dockhorn, Tim and Kim, Seung Wook and Fidler, Sanja and Kreis, Karsten}, booktitle={IEEE Conference on Computer Vision and Pattern Recognition ({CVPR})}, year={2023} }`\n",
            "```\n",
            "\n",
            "\n",
            "### Response:\n",
            "![](https://i.imgur.com/5yLXz9c.png)\n",
            "\n",
            "[![](https://i.imgur.com/5yLXz9c.png)](https://www.youtube.com/watch?v=4i9h6cG0u4E)\n",
            "\n",
            "## TL;DR\n",
            "\n",
            "TL;DR: We present Video Latent Diffusion Models (Video LDMs) for high-resolution video generation, achieving state-of-the-art performance in driving scene video synthesis and text-to-video generation. Our approach extends the LDM paradigm to video generation, mapping videos into a compressed latent space and modeling sequences of latent variables corresponding to video frames. We apply Video LDMs to two relevant real-world applications: Simulation of in-the-wild driving data and creative content creation with text-to-video modeling. We demonstrate synthesis of short videos of several seconds lengths with resolution up to 1280 x 2048, leveraging Stable Diffusion as backbone image LDM as well as the Stable Diffusion upscaler. We also explore the convolutional-in-time application of our models as an alternative approach to extend the length of videos. Our main keyframe models only train the newly inserted temporal layers, but do not touch the layers of the backbone image LDM. Because of that\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[128000,  39314,    374,  ...,   9393,    315,    430]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(dataset[\"validation\"][0][\"instruction\"], streaming=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "nB0VAp7CfLeH",
        "outputId": "dd5fa0f9-e81f-43a3-e158-4b49d43fdb83"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nYou are a helpful assistant specialized in summarizing documents. Generate a concise TL;DR summary in markdown format having a maximum of 512 characters of the key findings from the provided documents, highlighting the most significant insights\\n\\n### Input:\\n[![](assets/figures/nvidia.svg) Toronto AI Lab  ](https://research.nvidia.com/labs/toronto-ai/)\\n\\n# \\n\\nAlign your Latents:High-Resolution Video Synthesis with Latent Diffusion Models\\n\\n[Andreas Blattmann1 *,â€ ](https://twitter.com/andi_blatt) [Robin Rombach1 *,â€ ](https://twitter.com/robrombach) [Huan Ling2,3,4 *](https://www.cs.toronto.edu/~linghuan/) [Tim Dockhorn2,3,5 *,â€ ](https://timudk.github.io/) [Seung Wook Kim2,3,4](https://seung-kim.github.io/seungkim/) [Sanja Fidler2,3,4](https://www.cs.toronto.edu/~fidler/) [Karsten Kreis2](https://karstenkreis.github.io/)\\n\\n1 LMU Munich, 2 NVIDIA, 3 Vector Institute, 4 University of Toronto, 5 University of Waterloo\\n\\n* Equal contribution. â€  Andreas, Robin and Tim did the work during internships at NVIDIA.\\n\\n**IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2023**\\n\\n[Read the paper](https://arxiv.org/abs/2304.08818)\\n\\nTwo pandas discussing an academic paper.\\n\\nA storm trooper vacuuming the beach.\\n\\nAn animated painting of fluffy white clouds moving in sky.\\n\\nFireworks.\\n\\nTurtle swimming in ocean.\\n\\nA squirrel eating a burger.\\n\\nBeer pouring into glass, low angle video shot.\\n\\nSunset time lapse at the beach with moving clouds and colors in the sky, 4k, high resolution.\\n\\nAn astronaut feeding ducks on a sunny afternoon, reflection from the water.\\n\\nA cat wearing sunglasses and working as a lifeguard at a pool.\\n\\nClose up of grapes on a rotating table. High definition.\\n\\nAn astronaut flying in space, 4k, high resolution.\\n\\nFlying through fantasy landscapes, 4k, high resolution.\\n\\nA koala bear playing piano in the forest.\\n\\nA panda standing on a surfboard in the ocean in sunset, 4k, high resolution.\\n\\nA bigfoot walking in the snowstorm.\\n\\n[Click for more samples](samples.html)\\n\\n## Abstract\\n\\nLatent Diffusion Models (LDMs) enable high-quality image synthesis while avoiding excessive compute demands by training a diffusion model in a compressed lower-dimensional latent space. Here, we apply the LDM paradigm to high-resolution video generation, a particularly resource-intensive task. We first pre-train an LDM on images only; then, we turn the image generator into a video generator by introducing a temporal dimension to the latent space diffusion model and fine-tuning on encoded image sequences, i.e., videos. Similarly, we temporally align diffusion model upsamplers, turning them into temporally consistent video super resolution models. We focus on two relevant real-world applications: Simulation of in-the-wild driving data and creative content creation with text-to-video modeling. In particular, we validate our Video LDM on real driving videos of resolution 512 x 1024, achieving state-of-the-art performance. Furthermore, our approach can easily leverage off-the-shelf pre-trained image LDMs, as we only need to train a temporal alignment model in that case. Doing so, we turn the publicly available, state-of-the-art text-to-image LDM Stable Diffusion into an efficient and expressive text-to-video model with resolution up to 1280 x 2048. We show that the temporal layers trained in this way generalize to different fine-tuned text-to-image LDMs. Utilizing this property, we show the first results for personalized text-to-video generation, opening exciting directions for future content creation. \\n\\nYour browser does not support the video tag. \\n\\n**Animation of temporal video fine-tuning in our Video Latent Diffusion Models (Video LDMs).** We turn pre-trained image diffusion models into temporally consistent video generators. Initially, different samples of a batch synthesized by the model are independent. After temporal video fine-tuning, the samples are temporally aligned and form coherent videos. The stochastic generation processes before and after fine-tuning are visualised for a diffusion model of a one-dimensional toy distribution. For clarity, the figure corresponds to alignment in pixel space. In practice, we perform alignment in LDM\\'s latent space and obtain videos after applying LDM\\'s decoder. \\n\\n## Video Latent Diffusion Models\\n\\nWe present Video Latent Diffusion Models (Video LDMs) for computationally efficient high-resolution video generation. To alleviate the intensive compute and memory demands of high-resolution video synthesis, we leverage the LDM paradigm and extend it to video generation. Our Video LDMs map videos into a compressed latent space and model sequences of latent variables corresponding to the video frames (see animation above). We initialize the models from image LDMs and insert temporal layers into the LDMs\\' denoising neural networks to temporally model encoded video frame sequences. The temporal layers are based on temporal attention as well as 3D convolutions. We also fine-tune the model\\'s decoder for video generation (see figure below). \\n\\n![](./assets/figures/video_ldm_pipeline.png)\\n\\n**Latent diffusion model framework and video fine-tuning of decoder.** _Top:_ During temporal decoder fine-tuning, we process video sequences with a frozen per-frame encoder and enforce temporally coherent reconstructions across frames. We additionally employ a video-aware discriminator. _Bottom:_ in LDMs, a diffusion model is trained in latent space. It synthesizes latent features, which are then transformed through the decoder into images. Note that in practice we model entire videos and video fine-tune the latent diffusion model to generate temporally consistent frame sequences. \\n\\nOur Video LDM initially generates sparse keyframes at low frame rates, which are then temporally upsampled twice by another interpolation latent diffusion model. Moreover, optionally training Video LDMs for video prediction by conditioning on starting frames allows us to generate long videos in an autoregressive manner. To achieve high-resolution generation, we further leverage spatial diffusion model upsamplers and temporally align them for video upsampling. The entire generation stack is shown below. \\n\\n![](./assets/figures/video_ldm_stack.png)\\n\\n**Video LDM Stack.** We first generate sparse key frames. Then we temporally interpolate in two steps with the same interpolation model to achieve high frame rates. These operations use latent diffusion models (LDMs) that share the same image backbone. Finally, the latent video is decoded to pixel space and optionally a video upsampler diffusion model is applied. \\n\\n**Applications.** We validate our approach on two relevant but distinct applications: Generation of in-the-wild driving scene videos and creative content creation with text-to-video modeling. For driving video synthesis, our Video LDM enables generation of temporally coherent, multiple minute long videos at resolution 512 x 1024, achieving state-of-the-art performance. For text-to-video, we demonstrate synthesis of short videos of several seconds lengths with resolution up to 1280 x 2048, leveraging Stable Diffusion as backbone image LDM as well as the Stable Diffusion upscaler. We also explore the convolutional-in-time application of our models as an alternative approach to extend the length of videos. Our main keyframe models only train the newly inserted temporal layers, but do not touch the layers of the backbone image LDM. Because of that the learnt temporal layers can be transferred to other image LDM backbones, for instance to ones that have been fine-tuned with DreamBooth. Leveraging this property, we additionally show initial results for personalized text-to-video generation. \\n\\n## Text-to-Video Synthesis\\n\\nMany generated videos can be found at the top of the page as well as [**here**](samples.html). The generated videos have a resolution of 1280 x 2048 pixels, consist of 113 frames and are rendered at 24 fps, resulting in 4.7 second long clips. Our Video LDM for text-to-video generation is based on Stable Diffusion and has a total of 4.1B parameters, including all components except the CLIP text encoder. Only 2.7B of these parameters are trained on videos. This means that our models are significantly smaller than those of several concurrent works. Nevertheless, we can produce high-resolution, temporally consistent and diverse videos. This can be attributed to the efficient LDM approach. Below is another text-to-video sample, one of our favorites. \\n\\nYour browser does not support the video tag. \\n\\nText prompt: \"A teddy bear is playing the electric guitar, high definition, 4k.\" \\n\\n**Personalized Video Generation.** We insert the temporal layers that were trained for our Video LDM for text-to-video synthesis into image LDM backbones that we previously fine-tuned on a set of images following [**DreamBooth**](https://dreambooth.github.io/). The temporal layers generalize to the DreamBooth checkpoints, thereby enabling personalized text-to-video generation. \\n\\n![](./assets/text_to_video/dreambooth/cat_db.png)\\n\\nTraining images for DreamBooth. \\n\\nYour browser does not support the video tag. \\n\\nText prompt: \"A _**sks**_ cat playing in the grass.\" \\n\\nYour browser does not support the video tag. \\n\\nText prompt: \"A _**sks**_ cat getting up.\" \\n\\n![](./assets/text_to_video/dreambooth/opera_db.png)\\n\\nTraining images for DreamBooth. \\n\\nYour browser does not support the video tag. \\n\\nText prompt: \"A _**sks**_ building next to the Eiffel Tower.\" \\n\\nYour browser does not support the video tag. \\n\\nText prompt: \"Waves crashing against a _**sks**_ building, ominous lighting.\" \\n\\n![](./assets/text_to_video/dreambooth/frog_db.png)\\n\\nTraining images for DreamBooth. \\n\\nYour browser does not support the video tag. \\n\\nText prompt: \"A _**sks**_ frog playing a guitar in a band.\" \\n\\nYour browser does not support the video tag. \\n\\nText prompt: \"A _**sks**_ frog writing a scientific research paper.\" \\n\\n![](./assets/text_to_video/dreambooth/teapot_db.png)\\n\\nTraining images for DreamBooth. \\n\\nYour browser does not support the video tag. \\n\\nText prompt: \"A _**sks**_ tea pot floating in the ocean.\" \\n\\nYour browser does not support the video tag. \\n\\nText prompt: \"A _**sks**_ tea pot on top of a building in New York, drone flight, 4k.\" \\n\\n**Convolutional-in-Time Synthesis.** We also explored synthesizing slightly longer videos \"for free\" by applying our learnt temporal layers convolutionally in time. The below videos consist of 175 frames rendered at 24 fps, resulting in 7.3 second long clips. A minor degradation in quality can be observed. \\n\\nYour browser does not support the video tag. \\n\\nText prompt: \"Teddy bear walking down 5th Avenue, front view, beautiful sunset, close up, high definition, 4k.\" \\n\\nYour browser does not support the video tag. \\n\\nText prompt: \"Waves crashing against a lone lighthouse, ominous lighting.\" \\n\\n## Driving Scene Video Generation\\n\\nWe also train a Video LDM on in-the-wild real driving scene videos and generate videos at 512 x 1024 resolution. Here, we are additionally training prediction models to enable long video generation, allowing us to generate temporally coherent videos that are several minutes long. Below we show four short synthesized videos. Furthermore, several 5 minute long generated videos can be found [**here**](https://drive.google.com/file/d/1xlE079d4QmVZ-kWLZVsIk8iHWWH5wzKO/view?usp=share_link). \\n\\nYour browser does not support the video tag. \\n\\nYour browser does not support the video tag. \\n\\nYour browser does not support the video tag. \\n\\nYour browser does not support the video tag. \\n\\n**Specific Driving Scenario Simulation.** In practice, we may be interested in simulating a specific scene. To this end, we trained a bounding box-conditioned image-only LDM. Leveraging this model, we can place bounding boxes to construct a setting of interest, synthesize a corresponding starting frame, and then generate plausible videos starting from the designed scene. Below, the image on the left hand side is the initial frame that was generated based on the shown bounding boxes. On the right hand side, a video starting from that frame is generated. \\n\\n![](./assets/driving/bbox_scene_1.png)\\n\\nYour browser does not support the video tag. \\n\\n**Multimodal Driving Scenario Prediction.** As another potentially relevant application, we can take the same starting frame and generate multiple plausible rollouts. In the two sets of videos below, synthesis starts from the same initial frame. \\n\\nYour browser does not support the video tag. \\n\\nYour browser does not support the video tag. \\n\\n## Limitations\\n\\nThis is an NVIDIA research project, and the data sources used are for research purposes only and not intended for commercial application or use. \\n\\n## Paper\\n\\n[![](assets/figures/video_ldm_paper_preview.png)](https://arxiv.org/abs/2304.08818)\\n\\n**Align your Latents:High-Resolution Video Synthesis with Latent Diffusion Models**\\n\\nAndreas Blattmann*, Robin Rombach*, Huan Ling*, Tim Dockhorn*, Seung Wook Kim, Sanja Fidler, Karsten Kreis\\n\\n_* Equal contribution._\\n\\n_IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023_\\n\\ndescription [ arXiv version](https://arxiv.org/abs/2304.08818)\\n\\ninsert_comment [ BibTeX](assets/blattmann2023videoldm.bib)\\n\\n## Citation\\n\\n```\\n`@inproceedings{blattmann2023videoldm, title={Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models}, author={Blattmann, Andreas and Rombach, Robin and Ling, Huan and Dockhorn, Tim and Kim, Seung Wook and Fidler, Sanja and Kreis, Karsten}, booktitle={IEEE Conference on Computer Vision and Pattern Recognition ({CVPR})}, year={2023} }`\\n```\\n\\n\\n### Response:\\nTL;DR Summary:\\n\\n**TL;DR:** \\n[Align your Latents:High-Resolution Video Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2304.08818)\\n\\nThe paper introduces Video Latent Diffusion Models (Video LDMs) for efficient high-resolution video generation. It leverages the LDM paradigm and extends it to video generation by inserting temporal layers into the LDMs\\' denoising neural networks to model sequences of latent variables corresponding to video frames. The approach is validated on two applications: generation of in-the-wild driving scene videos and creative content creation with text-to-video modeling. \\n\\n[![Open in GitHub](https://raw.githubusercontent.com/andreasblatt/andreasblatt.github.io/master/images/github.png)](https://github.com/andreasblatt/andreasblatt.github.io)\\n\\n[![Open in GitHub](https://raw.githubusercontent.com/andreasblatt/andreasblatt.github.io/master/images/github.png)](https://github.com/andreasblatt/andreasblatt.github.io)\\n\\n[![Open in GitHub](https://raw.githubusercontent.com/andreasblatt/andreasblatt.github.io/master/images/github.png)](https://'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "\n",
        "## Saving Fine-tuned LLM\n",
        "\n",
        "The last step is to save the fine-tuned LLM locally and on Hugging Face if the Hugging Face token is available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "model_name = f\"{base_model}-Second-Brain-Summarization\"\n",
        "print(f\"Model name: {model_name}\")\n",
        "model.save_pretrained_merged(\n",
        "    model_name,\n",
        "    tokenizer,\n",
        "    save_method=\"merged_16bit\",\n",
        ")  # Local saving\n",
        "\n",
        "if enable_hf:\n",
        "    api = HfApi()\n",
        "    user_info = api.whoami(token=hf_token)\n",
        "    huggingface_user = user_info[\"name\"]\n",
        "    print(f\"Current Hugging Face user: {huggingface_user}\")\n",
        "\n",
        "    model.push_to_hub_merged(\n",
        "        f\"{huggingface_user}/{model_name}\",\n",
        "        tokenizer=tokenizer,\n",
        "        save_method=\"merged_16bit\",\n",
        "        token=hf_token,\n",
        "    )  # Online saving to Hugging Face"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uM4-5xMibwVF"
      },
      "source": [
        "\n",
        "And we're done! If you have any questions on Unsloth, they have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join their Discord!\n",
        "\n",
        "Also, you can visit their docs to learn about their supported [models](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n",
        "\n",
        "Some other links:\n",
        "1. Llama 3.2 Conversational notebook. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "4. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + â­ï¸ <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> â­ï¸\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "76190c73b3ef4adea553107e29eec382": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2cefba26286549b0a79bee0c49ad5718",
              "IPY_MODEL_324a04724d854acd8ce2f711ed78b61b",
              "IPY_MODEL_0d7fad603e784644b71263662efcc86c"
            ],
            "layout": "IPY_MODEL_e0d978b82000482782fb31b4d0fdb571"
          }
        },
        "2cefba26286549b0a79bee0c49ad5718": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_425114fe54594f2693231501a4872b70",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_23ebded49cf843ba876be4cc8148268e",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "324a04724d854acd8ce2f711ed78b61b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62f04293d83e43818e7c9a6c65aae64b",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d62fa06d371041108d746bdafa5409f6",
            "value": 4
          }
        },
        "0d7fad603e784644b71263662efcc86c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b6e61efaf4b496b8346a0c2e1247e4a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4bddcd59b2624581af78cb8c5970fa16",
            "value": "â€‡4/4â€‡[00:19&lt;00:00,â€‡â€‡4.18s/it]"
          }
        },
        "e0d978b82000482782fb31b4d0fdb571": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "425114fe54594f2693231501a4872b70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23ebded49cf843ba876be4cc8148268e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62f04293d83e43818e7c9a6c65aae64b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d62fa06d371041108d746bdafa5409f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7b6e61efaf4b496b8346a0c2e1247e4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bddcd59b2624581af78cb8c5970fa16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}