# ğŸš€ Installation and Usage Guide for the Second Brain Offline Module

This guide will help you set up and run the Second Brain Offline Module which contains the code for modules 1-5.

# ğŸ“‘ Table of Contents

- [ğŸ“‹ Prerequisites](#-prerequisites)
- [ğŸ¯ Getting Started](#-getting-started)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ—ï¸ Set Up Your Local Infrastructure](#-set-up-your-local-infrastructure)
- [âš¡ï¸ Running the Code for Each Lesson](#ï¸-running-the-code-for-each-lesson)
- [ğŸ”§ Utlity Commands](#-utility-commands)
- [ğŸ§Š Setup Notion (optional)](#-setup-notion-optional)

# ğŸ“‹ Prerequisites

## Local Tools

You'll need the following tools installed locally, starting with Module 1:

| Tool | Version | Purpose | Installation Link |
|------|---------|---------|------------------|
| Python | 3.11 | Programming language runtime | [Download](https://www.python.org/downloads/) |
| uv | â‰¥ 0.4.30 | Python package installer and virtual environment manager | [Download](https://github.com/astral-sh/uv) |
| GNU Make | â‰¥ 3.81 | Build automation tool | [Download](https://www.gnu.org/software/make/) |
| Git | â‰¥2.44.0 | Version control | [Download](https://git-scm.com/downloads) |
| Docker | â‰¥27.4.0 | Containerization platform | [Download](https://www.docker.com/get-started/) |

## Cloud Services

The project requires access to these cloud services:

| Service | Purpose | Cost | Required Credentials | Setup Guide | Starting with Module |
|---------|---------|------|---------------------|-------------| ---------------------|
| [OpenAI API](https://openai.com/index/openai-api/) | LLM API for recommender system | Pay-per-use | `OPENAI_API_KEY` | [Quick Start Guide](https://platform.openai.com/docs/quickstart) | Module 2 |
| [Hugging Face](https://huggingface.com/) | MLOps | Free tier | `HUGGINGFACE_ACCESS_TOKEN` | [Quick Start Guide](https://huggingface.co/docs/hub/en/security-tokens) | Module 3 |
| [Comet ML](https://www.comet.com/site/)  | Experiment tracking |  Free tier | `COMET_API_KEY` | [Quick Start Guide](https://www.comet.com/docs/v2/api-and-sdk/rest-api/overview/) | Module 4 |
| [Opik](https://www.comet.com/site/products/opik/) | LLM vvaluation and prompt monitoring | Free tier  | `COMET_API_KEY` | [Quick Start Guide](https://www.comet.com/docs/v2/api-and-sdk/rest-api/overview/) | Module 6 |

Other optional services in case you want to deploy the code. When working locally, you can use the default values found in the `config.py` file:

| Service | Purpose | Cost | Required Credentials | Setup Guide |
|---------|---------|------|---------------------|-------------| 
| [MongoDB](https://www.mongodb.com/) | NoSQL and vector database | Free tier | `MONGODB_URI` | 1. [Create a free MongoDB Atlas account](https://www.mongodb.com/cloud/atlas/register/?utm_campaign=paul_iusztin&utm_medium=referral) <br> 2. [Create a Cluster](https://www.mongodb.com/docs/guides/atlas/cluster/?utm_campaign=paul_iusztin&utm_medium=referral) </br> 3. [Add a Database User](https://www.mongodb.com/docs/guides/atlas/db-user/?utm_campaign=paul_iusztin&utm_medium=referral) </br> 4. [Configure a Network Connection](https://www.mongodb.com/docs/guides/atlas/network-connections/?utm_campaign=paul_iusztin&utm_medium=referral) |

# ğŸ¯ Getting Started

## 1. Clone the Repository

Start by cloning the repository and navigating to the project directory:
```
git clone https://github.com/decodingml/second-brain-ai-assistant-course.git
cd second-brain-ai-assistant-course 
```

## 2. Installation

First deactivate any active virtual environment and move to the `second-brain-online` directory:
```bash
deactivate
cd apps/second-brain-offline
```

To install the dependencies and activate the virtual environment, run the following commands:

```bash
uv venv .venv-offline
. ./.venv-offline/bin/activate # or source ./.venv-offline/bin/activate
uv pip install -e .
```

Finish setting up `Crew4AI` for crawling:
```bash
# Run post-installation setup
uv pip install -U "crawl4ai==0.4.247" # We have to upgrade crawl4ai to support these CLI commands (we couldn't add it to pyproject.toml due to ZenML version incompatibility with Pydantic).
crawl4ai-setup

# Verify your installation
crawl4ai-doctor
```

> [!IMPORTANT]
> As crawling can often fail, both during installation and while running the crawling logic, you can skip the crawling step and use our pre-computed dataset. More on this in the [Running the ML pipelines / Lessons](#running-the-ml-pipelines--lessons) section.

After running the doctor command, you should see something like this:
```console
[INIT].... â†’ Running Crawl4AI health check...
[INIT].... â†’ Crawl4AI 0.4.247
[TEST].... â„¹ Testing crawling capabilities...
[EXPORT].. â„¹ Exporting PDF and taking screenshot took 0.84s
[FETCH]... â†“ https://crawl4ai.com... | Status: True | Time: 3.91s
[SCRAPE].. â—† Processed https://crawl4ai.com... | Time: 11ms
[COMPLETE] â— https://crawl4ai.com... | Status: True | Total: 3.92s
[COMPLETE] â— âœ… Crawling test passed!
```
[More on installing Crawl4AI](https://docs.crawl4ai.com/core/installation/)

## 3. Environment Configuration

Before running any components:
1. Create your environment file:
   ```bash
   cp .env.example .env
   ```
2. Open `.env` and configure the required credentials following the inline comments and the recommendations from the [Cloud Services](#-prerequisites) section.

# ğŸ“ Project Structure

At Decoding ML we teach how to build production ML systems, thus the course follows the structure of a real-world Python project:

```bash
.
â”œâ”€â”€ configs/                   # ZenML configuration files
â”œâ”€â”€ pipelines/                 # ZenML ML pipeline definitions
â”œâ”€â”€ src/second_brain_offline/  # Main package directory
â”‚   â”œâ”€â”€ application/           # Application layer
â”‚   â”œâ”€â”€ domain/                # Domain layer
â”‚   â”œâ”€â”€ infrastructure/        # Infrastructure layer
â”‚   â”œâ”€â”€ config.py              # Configuration settings
â”‚   â””â”€â”€ utils.py               # Utility functions
â”œâ”€â”€ steps/                     # ZenML pipeline steps
â”œâ”€â”€ tests/                     # Test files
â”œâ”€â”€ tools/                     # Entrypoint scripts that use the Python package
â”œâ”€â”€ .env.example               # Environment variables template
â”œâ”€â”€ .python-version            # Python version specification
â”œâ”€â”€ Makefile                   # Project commands
â””â”€â”€ pyproject.toml             # Project dependencies
```

# ğŸ—ï¸ Set Up Your Local Infrastructure

To start the local infrastructure (ZenML, MongoDB):
```bash
make local-infrastructure-up
```

To stop the local infrastructure (ZenML, MongoDB):
```bash
make local-infrastructure-down
```

# âš¡ï¸ Running the Code for Each Module

## Module 1: Build your Second Brain AI assistant

Lesson: [Build your Second Brain AI assistant]()

No code to run for this lesson. Read the lesson to understand the problem and overall architecture of the Second Brain AI assistant.

## Module 2: ETL pipeline

### Prepare Notion data

Download our prepared Notion dataset from S3 (recommended):
```bash
make download-notion-dataset
# Validate using test: make test-download-notion-dataset
```

Or if you want to prepare your own Notion data (optional - if you want to use your own data):
```bash
make collect-notion-data-pipeline
```

### Run the ETL pipeline

Run the ETL pipeline to crawl, score and ingest the Notion data into MongoDB:
```bash
make etl-pipeline
# Validate using test: make test-etl-pipeline
```
Running costs: ~$0.5 </br>
Running time: ~30 minutes

If you want to avoid any costs or waiting times, you can use our pre-computed dataset to populate MongoDB. Also, as crawling can often fail, you can use this dataset to skip the crawling step:
```bash
make download-crawled-dataset
# Validate using test: make test-download-crawled-dataset
make etl-precomputed-pipeline
```

## Module 3: Generate Fine-tuning Dataset

```bash
make generate-dataset-pipeline
```
Running costs: ~$1.5 </br>
Running time: ~60 minutes

In case you want to avoid any costs or waiting times, you can use our pre-computed dataset available on Hugging Face, which is already set as default in future steps: [pauliusztin/second_brain_course_summarization_task](https://huggingface.co/datasets/pauliusztin/second_brain_course_summarization_task).

## Lesson 4: Fine-tuning and Evaluating Summarization LLM

This time we will use Notebooks, as they are popular when it comes to LLM fine-tuning.

| Purpose | Notebook | Useful Resources |
|---------|----------|------------------|
| Training | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/decodingml/second-brain-ai-assistant-course/blob/main/apps/second-brain-offline/src/second_brain_offline/application/models/finetuning.ipynb) | [Tracking training with an experiment tracker](https://www.comet.com/iusztinpaul/second-brain-course/f142e308313b4eedb298f15a34f5f0bb?compareXAxis=step&experiment-tab=panels&showOutliers=true&smoothing=0&xAxis=step) |
| Inference | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/decodingml/second-brain-ai-assistant-course/blob/main/apps/second-brain-offline/src/second_brain_offline/application/models/inference.ipynb) | - |
| Evaluation | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/decodingml/second-brain-ai-assistant-course/blob/main/apps/second-brain-offline/src/second_brain_offline/application/models/evaluate.ipynb) | - |


## Lesson 5: Compute RAG vector index

```bash
make compute-rag-vector-index-pipeline
```

## Lesson 6: Agentic App

The Agentic App sits in the online environment, which is implemented as a different Python application.

Go to the [apps/second-brain-online](../second-brain-online/) folder and follow the instructions there to set it up and run it.

# ğŸ”§ Utlity Commands

## Formatting

```
make format-check
make format-fix
```

## Linting

```bash
make lint-check
make lint-fix
```

## Tests

```bash
make test
```

# ğŸ§Š Setup Notion (optional)

In case you want to use your own Notion data, you can follow these steps to set up an integration and read from your Notion database:

1. Go to [https://www.notion.so/profile].
2. Create an integration following [this tutorial](https://developers.notion.com/docs/authorization).
3. Copy your integration secret to programatically read from Notion.
4. Share your database with the integration:
   - Open your Notion database
   - Click the '...' menu in the top right
   - Click 'Add connections'
   - Select your integration
5. Get the correct database ID:
   - Open your database in Notion
   - Copy the ID from the URL: 
     ```
     https://www.notion.so/{workspace}/{database_id}?v={view_id}
     ```
   - The database ID is the part between the workspace name and the question mark
