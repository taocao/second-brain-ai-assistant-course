# Notes



<child_page>
# Agents Memory

ğŸ”—Â Source: [https://www.linkedin.com/posts/aurimas-griciunas_llm-ai-machinelearning-activity-7284520203705610240-gkSj?utm_source=share&utm_medium=member_desktop](https://www.linkedin.com/posts/aurimas-griciunas_llm-ai-machinelearning-activity-7284520203705610240-gkSj?utm_source=share&utm_medium=member_desktop)
---

A simple way to explain ğ—”ğ—œ ğ—”ğ—´ğ—²ğ—»ğ˜ ğ— ğ—²ğ—ºğ—¼ğ—¿ğ˜†.
In general, the memory for an agent is something that we provide via context in the prompt passed to LLM that helps the agent to better plan and react given past interactions or data not immediately available.
It is useful to group the memory into four types:
ğŸ­. Episodic - This type of memory contains past interactions and actions performed by the agent. After an action is taken, the application controlling the agent would store the action in some kind of persistent storage so that it can be retrieved later if needed. A good example would be using a vector Database to store semantic meaning of the interactions.
ğŸ®. Semantic - Any external information that is available to the agent and any knowledge the agent should have about itself. You can think of this as a context similar to one used in RAG applications. It can be internal knowledge only available to the agent or a grounding context to isolate part of the internet scale data for more accurate answers.
ğŸ¯. Procedural - This is systemic information like the structure of the System Prompt, available tools, guardrails etc. It will usually be stored in Git, Prompt and Tool Registries.
ğŸ°. Occasionally, the agent application would pull information from long-term memory and store it locally if it is needed for the task at hand.
ğŸ±. All of the information pulled together from the long-term or stored in local memory is called short-term or working memory. Compiling all of it into a prompt will produce the prompt to be passed to the LLM and it will provide further actions to be taken by the system.
We usually label 1. - 3. as Long-Term memory and 5. as Short-Term memory.
A visual explanation of potential implementation details ğŸ‘‡
And that is it! The rest is all about how you architect the flow of your Agentic systems.
[Image](No URL)
</child_page>




---

# Community

	[https://github.com/neural-maze/agentic_patterns](https://github.com/neural-maze/agentic_patterns)
	[Prompt Engineering Guide](https://www.promptingguide.ai/)
	[LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent)
	[LLM Agents](https://www.promptingguide.ai/research/llm-agents)
	[https://huyenchip.com//2025/01/07/agents.html](https://huyenchip.com//2025/01/07/agents.html)
	[https://github.com/coleam00/ai-agents-masterclass/tree/main](https://github.com/coleam00/ai-agents-masterclass/tree/main)

# Science

	[Chain of Thought](https://arxiv.org/abs/2201.11903?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8TWMQ2pzYlyupoha6NJn2_c8a9NVXjbrj_SXljxGjznmQTE8OZx9MLwfZlDobYLwnqPJjN)Â prompting, which asks LLMs to think step by step
	[Self-consistency](https://arxiv.org/abs/2203.11171?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8TWMQ2pzYlyupoha6NJn2_c8a9NVXjbrj_SXljxGjznmQTE8OZx9MLwfZlDobYLwnqPJjN), which prompts a model to generate several responses and pick the one thatâ€™s most consistent with the others
	[ReAc](https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8TWMQ2pzYlyupoha6NJn2_c8a9NVXjbrj_SXljxGjznmQTE8OZx9MLwfZlDobYLwnqPJjN)t, which interleaves reasoning and action steps to accomplish a goal
	[Self-Refine](https://arxiv.org/abs/2303.17651?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8TWMQ2pzYlyupoha6NJn2_c8a9NVXjbrj_SXljxGjznmQTE8OZx9MLwfZlDobYLwnqPJjN), which enables an agent to reflect on its own output
	[Reflexion](https://arxiv.org/abs/2303.11366?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8TWMQ2pzYlyupoha6NJn2_c8a9NVXjbrj_SXljxGjznmQTE8OZx9MLwfZlDobYLwnqPJjN), which enables a model to act, evaluate, reflect, and repeat.
	[Test-time compute](https://arxiv.org/abs/2408.03314?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-8TWMQ2pzYlyupoha6NJn2_c8a9NVXjbrj_SXljxGjznmQTE8OZx9MLwfZlDobYLwnqPJjN), which increases the amount of processing power allotted to inference

# Tools

	- [AgentGPT](https://github.com/reworkd/AgentGPT)
	- [AutoGPT](https://github.com/Significant-Gravitas/AutoGPT)
	- [babyagi](https://github.com/yoheinakajima/babyagi)
	- [https://github.com/gpt-engineer-org/gpt-engineer](https://github.com/gpt-engineer-org/gpt-engineer)
	
	[https://github.com/huggingface/smolagents](https://github.com/huggingface/smolagents)
	[https://github.com/pydantic/pydantic-ai](https://github.com/pydantic/pydantic-ai)
	[https://docs.llamaindex.ai/en/stable/module_guides/workflow/](https://docs.llamaindex.ai/en/stable/module_guides/workflow/)
	[https://www.langchain.com/langgraph](https://www.langchain.com/langgraph)
	[https://github.com/crewAIInc/crewAI](https://github.com/crewAIInc/crewAI)